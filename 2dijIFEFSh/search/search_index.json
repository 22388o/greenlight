{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Greenlight a Blockstream project offering hosted, non-custodial, Lightning Network nodes to end-users. We take care of the infrastructure, while you are in control of the keys. Nodes can be registered and started in seconds, perfect for new users that are just starting their Bitcoin and Lightning Network journey, developers wanting to provide their users with access to the Lightning Network, or hobbyists wanting to hack tools for their own use. It is not our goal to capture users: once you've gained experience running a node, you can off-board into your own infrastructure, and we'll teach you how to do that too.","title":"Introduction"},{"location":"#introduction","text":"Greenlight a Blockstream project offering hosted, non-custodial, Lightning Network nodes to end-users. We take care of the infrastructure, while you are in control of the keys. Nodes can be registered and started in seconds, perfect for new users that are just starting their Bitcoin and Lightning Network journey, developers wanting to provide their users with access to the Lightning Network, or hobbyists wanting to hack tools for their own use. It is not our goal to capture users: once you've gained experience running a node, you can off-board into your own infrastructure, and we'll teach you how to do that too.","title":"Introduction"},{"location":"about/","text":"About","title":"About"},{"location":"about/#about","text":"","title":"About"},{"location":"getting-started/","text":"Overview Before diving into the specifics, let's first define a number of concepts that will be useful once we start developing: Nodes Greenlight provisions and manages Core Lightning nodes on behalf of its users. The nodes expose the grpc interface defined in the cln-grpc proto file , without limitations. The goal of this guide is to spin up a node and interact with it as if it were a local Core Lightning node. Authentication All communication channels in Greenlight are authenticated and encrypted via mTLS (mutual Transport Layer Security), similar to HTTPS in the browser. Each client receives its own identity in the form of a private key and matching certificate, which can then be used to authenticate and encrypt communication when talking with Greenlight. In this guide we will be using two identities: nobody : An identity that is shipped with the library, used to communicate with services that don't require authentication. A Device Identity: A unique identity used by the application to authenticate with the Greenlight. The private key is generated and kept locally, and certified by Greenlight to belong to a given node. See the security page for more details about how the authentication works. Signer The signer manages any private information, is used to prove node ownership when registering and recovering, and processes signature requests from the node. It is initialized with the secret seed (a 32 byte secret), the bitcoin network the node runs on, and the identity to use when communicating with the node. See the security page for details on how the signer ensures that operations it signs off originate from an authenticated app. Scheduler Node in Greenlight are scheduled on-demand when a client needs to talk to it. The Scheduler tracks which nodes are running where, and starts them if they aren't running yet. You can think of it as just a mechanism to register new nodes and look up where they are running. Seed Secret Signer Scheduler","title":"Overview"},{"location":"getting-started/#overview","text":"Before diving into the specifics, let's first define a number of concepts that will be useful once we start developing:","title":"Overview"},{"location":"getting-started/#nodes","text":"Greenlight provisions and manages Core Lightning nodes on behalf of its users. The nodes expose the grpc interface defined in the cln-grpc proto file , without limitations. The goal of this guide is to spin up a node and interact with it as if it were a local Core Lightning node.","title":"Nodes"},{"location":"getting-started/#authentication","text":"All communication channels in Greenlight are authenticated and encrypted via mTLS (mutual Transport Layer Security), similar to HTTPS in the browser. Each client receives its own identity in the form of a private key and matching certificate, which can then be used to authenticate and encrypt communication when talking with Greenlight. In this guide we will be using two identities: nobody : An identity that is shipped with the library, used to communicate with services that don't require authentication. A Device Identity: A unique identity used by the application to authenticate with the Greenlight. The private key is generated and kept locally, and certified by Greenlight to belong to a given node. See the security page for more details about how the authentication works.","title":"Authentication"},{"location":"getting-started/#signer","text":"The signer manages any private information, is used to prove node ownership when registering and recovering, and processes signature requests from the node. It is initialized with the secret seed (a 32 byte secret), the bitcoin network the node runs on, and the identity to use when communicating with the node. See the security page for details on how the signer ensures that operations it signs off originate from an authenticated app.","title":"Signer"},{"location":"getting-started/#scheduler","text":"Node in Greenlight are scheduled on-demand when a client needs to talk to it. The Scheduler tracks which nodes are running where, and starts them if they aren't running yet. You can think of it as just a mechanism to register new nodes and look up where they are running. Seed Secret Signer Scheduler","title":"Scheduler"},{"location":"getting-started/installation/","text":"Installing the library Greenlight provides client libraries for a variety of programming languages, providing an idiomatic interface to developers. The libraries allow interaction with both the Scheduler and the Node . The Scheduler provides access to the node metadata, while the Node is the user's CLN node running on Greenlight's infrastructure. Steps to install the library depend on the programming language and target environment. The code blocks below provide tabs for the most common ones: Rust Python Javascript Add the gl-client crate as a dependency: [dependencies] gl-client = { git = \"ssh://git@github.com/Blockstream/greenlight\" } The python library currently resides on a private repository which has to be specified during the installation: pip install \\ --extra-index-url = https://us-west2-python.pkg.dev/c-lightning/greenlight-pypi/simple/ \\ -U glcli The javascript library currenctly resides on a private repository, which needs to be configured the first time you install it: npm config set @greenlight:registry = https://us-west2-npm.pkg.dev/c-lightning/test-npm/ Afterwards specifying the namespace @greenlight is sufficient to install from the private repository. npm install @greenlight/gl-client-js","title":"Installation"},{"location":"getting-started/installation/#installing-the-library","text":"Greenlight provides client libraries for a variety of programming languages, providing an idiomatic interface to developers. The libraries allow interaction with both the Scheduler and the Node . The Scheduler provides access to the node metadata, while the Node is the user's CLN node running on Greenlight's infrastructure. Steps to install the library depend on the programming language and target environment. The code blocks below provide tabs for the most common ones: Rust Python Javascript Add the gl-client crate as a dependency: [dependencies] gl-client = { git = \"ssh://git@github.com/Blockstream/greenlight\" } The python library currently resides on a private repository which has to be specified during the installation: pip install \\ --extra-index-url = https://us-west2-python.pkg.dev/c-lightning/greenlight-pypi/simple/ \\ -U glcli The javascript library currenctly resides on a private repository, which needs to be configured the first time you install it: npm config set @greenlight:registry = https://us-west2-npm.pkg.dev/c-lightning/test-npm/ Afterwards specifying the namespace @greenlight is sufficient to install from the private repository. npm install @greenlight/gl-client-js","title":"Installing the library"},{"location":"getting-started/recover/","text":"","title":"Recover access"},{"location":"getting-started/register/","text":"Register a node Preparing a node identity We start by creating a node identity, consisting of a node's seed secret, and it's mTLS certificates we'll later use to authenticate against Greenlight. Let's start with the seed secret: the seed secret is a 32 byte secret that all other secrets and private keys are derived from, as such it is paramount that this secret never leaves your device and is only handled by the Signer . It is suggested to derive the seed secret from a BIP 39 seed phrase, so the user can back it up on a physical piece of paper, steel plate, or whatever creative way of storing it they can think of. Note The following code-snippets build on each other. By copying each snippet after the other you should get a working example. Rust Python Install the bip39 and rand crates required for secure randomness and conversion of mnemonics. Add the following lines to your Cargo.toml [dependencies] rand = \"*\" bip39 = \"*\" Install the bip39 package which we'll use to encode the seed secret as a seed phrase: pip install bip39 Now we can securely generate some randomness, encode it as BIP 39 phrase and then convert it into a seed secret we can use: Rust Python Javascript use bip39 :: { Mnemonic , Language }; let mut rng = rand :: thread_rng (); let m = Mnemonic :: generate_in_with ( & mut rng , Language :: English , 24 ). unwrap (); let phrase = m . word_iter (). fold ( \"\" . to_string (), | c , n | c + \" \" + n ); # Prompt user to safely store the phrase seed = m . to_seed ( \"\" )[ 0 .. 32 ] # Only need the first 32 bytes # Store the seed on the filesystem , or secure configuration system import bip39 import secrets # Make sure to use cryptographically sound randomness rand = secres . randbits ( 256 ) # 32 bytes of randomness phrase = bip39 . encode_bytes ( rand ) # Prompt user to safely store the phrase seed = bip32 . phrase_to_seed ( phrase )[: 32 ] # Only need 32 bytes # Store the seed on the filesystem, or secure configuration system Remember to store the seed somewhere (file on disk, registry, etc) because without it, you will not have access to the node, and any funds on the node will be lost forever! We mean it when we say you're the only one with access to the seed ! Next we instantiate an mTLS identity we will use to authenticate with Greenlight. Since we haven't registered our node with the service yet, we will use a dummy key, that allows us to talk to the Scheduler but will not allow us to talk to any other service. No worries, once we register the node we will get a valid certificate to authenticate. Rust Python Javascript use gl_client :: tls :: TlsConfig ; let tls = TlsConfig :: new (); from glclient import TlsConfig # Creating a new `TlsConfig` object will automatically load the dummy identity tls = TlsConfig () const glclient = require ( 'glclient' ); # Creating a new `TlsConfig` object will automatically load the dummy identity var tls = new glclient . TlsConfig (); Finally, we can create the Signer which processes incoming signature requests, and is used when registering a node to prove ownership of the private key. The last thing to decide is which network we want the node to run on. You can chose between the following networks: testnet bitcoin We'll pick bitcoin , because ... reckless \ud83d\ude09 Rust Python Javascript use gl_client :: signer :: Signer ; use bitcoin :: Network ; signer = Signer :: new ( secret , Network :: Bitcoin , tls ); from glclient import Signer signer = Signer ( seed , network = \"bitcoin\" , tls = tls ) Registering a new node Registering a node with the Scheduler creates the node metadata on the Greenlight service, including the node's identity and the public key, and ensures everything is setup to start the node. In order to register a node, the client needs to prove it has access to the corresponding private key. Since the private key is managed exclusively by the Signer we need to first instantiate that: Python from glclient import Scheduler scheduler = Scheduler ( node_id = signer . node_id (), network = \"bitcoin\" , tls = tls , ) # Passing in the signer is required because the client needs to prove # ownership of the `node_id` res = scheduler . register ( signer , invite_code = invite_code ) The result of register contains the credentials that can be used going forward to talk to the scheduler and the node itself. Please make sure to store them somewhere safe, since anyone with these credentials can access your node. Rust Python let tls = TlsConfig().identity(res.device_cert, res.device_key); // Use the configured `tls` instance when creating `Scheduler` and `Signer` // instance going forward signer = Signer(seed, Network::Bitcoin, tls); scheduler = Scheduler::with(signer.node_id(), Network::Bitcoin, tls).await?; tls = TlsConfig () . with_identity ( res . device_cert , res . device_key ) # Use the configured `tls` instance when creating `Scheduler` and `Signer` # instance going forward signer = Signer ( seed , network = \"bitcoin\" , tls = tls ) node = Scheduler ( node_id = signer . node_id (), network = \"bitcoin\" , tls = tls ) . node () If you get an error about a certificate verification failure when talking to the node, you most likely are using an unconfigured TlsConfig that doesn't have access to the node. See Security for details on how authentication and authorization work under the hood.","title":"Registering a node"},{"location":"getting-started/register/#register-a-node","text":"","title":"Register a node"},{"location":"getting-started/register/#preparing-a-node-identity","text":"We start by creating a node identity, consisting of a node's seed secret, and it's mTLS certificates we'll later use to authenticate against Greenlight. Let's start with the seed secret: the seed secret is a 32 byte secret that all other secrets and private keys are derived from, as such it is paramount that this secret never leaves your device and is only handled by the Signer . It is suggested to derive the seed secret from a BIP 39 seed phrase, so the user can back it up on a physical piece of paper, steel plate, or whatever creative way of storing it they can think of. Note The following code-snippets build on each other. By copying each snippet after the other you should get a working example. Rust Python Install the bip39 and rand crates required for secure randomness and conversion of mnemonics. Add the following lines to your Cargo.toml [dependencies] rand = \"*\" bip39 = \"*\" Install the bip39 package which we'll use to encode the seed secret as a seed phrase: pip install bip39 Now we can securely generate some randomness, encode it as BIP 39 phrase and then convert it into a seed secret we can use: Rust Python Javascript use bip39 :: { Mnemonic , Language }; let mut rng = rand :: thread_rng (); let m = Mnemonic :: generate_in_with ( & mut rng , Language :: English , 24 ). unwrap (); let phrase = m . word_iter (). fold ( \"\" . to_string (), | c , n | c + \" \" + n ); # Prompt user to safely store the phrase seed = m . to_seed ( \"\" )[ 0 .. 32 ] # Only need the first 32 bytes # Store the seed on the filesystem , or secure configuration system import bip39 import secrets # Make sure to use cryptographically sound randomness rand = secres . randbits ( 256 ) # 32 bytes of randomness phrase = bip39 . encode_bytes ( rand ) # Prompt user to safely store the phrase seed = bip32 . phrase_to_seed ( phrase )[: 32 ] # Only need 32 bytes # Store the seed on the filesystem, or secure configuration system Remember to store the seed somewhere (file on disk, registry, etc) because without it, you will not have access to the node, and any funds on the node will be lost forever! We mean it when we say you're the only one with access to the seed ! Next we instantiate an mTLS identity we will use to authenticate with Greenlight. Since we haven't registered our node with the service yet, we will use a dummy key, that allows us to talk to the Scheduler but will not allow us to talk to any other service. No worries, once we register the node we will get a valid certificate to authenticate. Rust Python Javascript use gl_client :: tls :: TlsConfig ; let tls = TlsConfig :: new (); from glclient import TlsConfig # Creating a new `TlsConfig` object will automatically load the dummy identity tls = TlsConfig () const glclient = require ( 'glclient' ); # Creating a new `TlsConfig` object will automatically load the dummy identity var tls = new glclient . TlsConfig (); Finally, we can create the Signer which processes incoming signature requests, and is used when registering a node to prove ownership of the private key. The last thing to decide is which network we want the node to run on. You can chose between the following networks: testnet bitcoin We'll pick bitcoin , because ... reckless \ud83d\ude09 Rust Python Javascript use gl_client :: signer :: Signer ; use bitcoin :: Network ; signer = Signer :: new ( secret , Network :: Bitcoin , tls ); from glclient import Signer signer = Signer ( seed , network = \"bitcoin\" , tls = tls )","title":"Preparing a node identity"},{"location":"getting-started/register/#registering-a-new-node","text":"Registering a node with the Scheduler creates the node metadata on the Greenlight service, including the node's identity and the public key, and ensures everything is setup to start the node. In order to register a node, the client needs to prove it has access to the corresponding private key. Since the private key is managed exclusively by the Signer we need to first instantiate that: Python from glclient import Scheduler scheduler = Scheduler ( node_id = signer . node_id (), network = \"bitcoin\" , tls = tls , ) # Passing in the signer is required because the client needs to prove # ownership of the `node_id` res = scheduler . register ( signer , invite_code = invite_code ) The result of register contains the credentials that can be used going forward to talk to the scheduler and the node itself. Please make sure to store them somewhere safe, since anyone with these credentials can access your node. Rust Python let tls = TlsConfig().identity(res.device_cert, res.device_key); // Use the configured `tls` instance when creating `Scheduler` and `Signer` // instance going forward signer = Signer(seed, Network::Bitcoin, tls); scheduler = Scheduler::with(signer.node_id(), Network::Bitcoin, tls).await?; tls = TlsConfig () . with_identity ( res . device_cert , res . device_key ) # Use the configured `tls` instance when creating `Scheduler` and `Signer` # instance going forward signer = Signer ( seed , network = \"bitcoin\" , tls = tls ) node = Scheduler ( node_id = signer . node_id (), network = \"bitcoin\" , tls = tls ) . node () If you get an error about a certificate verification failure when talking to the node, you most likely are using an unconfigured TlsConfig that doesn't have access to the node. See Security for details on how authentication and authorization work under the hood.","title":"Registering a new node"},{"location":"getting-started/schedule/","text":"Starting a node Now that the node has been registered on the Greenlight server, we can schedule it. Scheduling will tell the scheduler that we want to interact with the node, and need its GRPC URI, so we can talk to it. The scheduler will look up the node, check if it is currently running, and if not it'll start the node. It will then return the URL you can use to connect to the node directly. Important Currently nodes will get a new address whenever they are started, so don't cache the URL for longer periods of time. We spin nodes down if there is no client talking to it, and slots are reused for other nodes. Attempting to talk to a node that isn't yours will fail to establish a connection. The Greenlight team is working on an improvement that will assign a unique address to each node, ensuring that you always know how to reach the node, and allowing you to skip talking with the scheduler altogether. First of all we build an instance of the scheduler service stub, which will allow us to call methods on the service. We then schedule the node, which returns a stub representing the node running on the Greenlight infrastructure: Rust Python use hex ; let node_id = hex :: decode ( \"02058e8b6c2ad363ec59aa136429256d745164c2bdc87f98f0a68690ec2c5c9b0b\" ) ? ; let network = \"testnet\" ; let scheduler = gl_client :: scheduler :: Scheduler ( node_id , network ) ? ; let node : gl_client :: node :: ClnClient = scheduler . schedule () ? ; from glclient import TlsConfig , Scheduler , cert , key = b '...' , b '...' node_id = bytes . fromhex ( \"02058e8b6c2ad363ec59aa136429256d745164c2bdc87f98f0a68690ec2c5c9b0b\" ) network = \"testnet\" tls = TlsConfig . with_identity ( cert , key ) scheduler = Scheduler ( node_id , network , tls ) node = scheduler . node () Once we have an instance of the Node we can start interacting with it via the GRPC interface: Rust Python use gl_client :: pb :: cln ; let info = node . get_info ( cln :: GetinfoRequest :: default ()). await ? ; let peers = node . list_peers ( gl_client :: pb :: cln :: ListpeersRequest :: default ()). await ? ; info = node . get_into () peers = node . list_peers () The above snippet will read the metadata and list the peers from the node. Both of these are read-only operations, that do not require a signer to sign off. What happens if we issue a command that requires a signer to sign off? Let's try to connect to create an invoice. Invoices are signed using the node key, and the signer is the only component with access to your key. Rust Python node . invoice ( cln :: InvoiceRequest { label : \"label\" . to_string (), description : \"description\" . to_string (), .. Default :: default (), }). await ? ; from glclient import clnpb node . invoice ( amount_msat = clnpb . AmountOrAny ( any = True ), label = \"label\" , description = \"description\" , ) You'll notice that these calls hang indefinitely. This is because the signer is not running and not attached to the node, and without its signature we can't create the invoice. This isn't just the case for the invoice call either, all calls that somehow use the Node ID, or move funds, will require the signer's sign-off. You can think of a node without a signer being connected as a read-only node, and as soon as you attach the signer, the node becomes fully functional. So how do we attach the signer? Simple: load the secret from where you stored it in the last chapter, instantiate the signer with it and then start it. Rust Python let seed = .. . // Load from wherever you stored it let ( cert , key ) = .. . // Load the cert and key you got from the `register` call // The signer task will run until we send a shutdown signal on this channel let ( tx , mut rx ) = tokio :: mpsc :: channel ( 1 ); let tls = TlsConfig (). identity ( cert , key ); signer = Signer ( seed , Network :: Bitcoin , tls ); signer . run_forever ( rx ). await ? ; Notice that signer.run_forever() returns a Future which you can spawn a new task with. That is also the reason why a separate shutdown signal is provided. seed = ... # Load from wherever you stored it cert , key = ... // Load the cert and key you got from the ` register ` call tls = TlsConfig () . with_identity ( res . device_cert , res . device_key ) signer = Signer :: new ( secret , Network :: Bitcoin , tls ) signer . run_in_thread () If you kept the stuck commands above running, you should notice that they now return a result. As mentioned before many RPC calls will need the signer to be attached to the node, so it's best to just start it early, and keep it running in the background whenever possible. The signer will not schedule the node by itself, instead waiting on the scheduler, so it doesn't consume much resources, but still be available when it is needed.","title":"Starting a node"},{"location":"getting-started/schedule/#starting-a-node","text":"Now that the node has been registered on the Greenlight server, we can schedule it. Scheduling will tell the scheduler that we want to interact with the node, and need its GRPC URI, so we can talk to it. The scheduler will look up the node, check if it is currently running, and if not it'll start the node. It will then return the URL you can use to connect to the node directly. Important Currently nodes will get a new address whenever they are started, so don't cache the URL for longer periods of time. We spin nodes down if there is no client talking to it, and slots are reused for other nodes. Attempting to talk to a node that isn't yours will fail to establish a connection. The Greenlight team is working on an improvement that will assign a unique address to each node, ensuring that you always know how to reach the node, and allowing you to skip talking with the scheduler altogether. First of all we build an instance of the scheduler service stub, which will allow us to call methods on the service. We then schedule the node, which returns a stub representing the node running on the Greenlight infrastructure: Rust Python use hex ; let node_id = hex :: decode ( \"02058e8b6c2ad363ec59aa136429256d745164c2bdc87f98f0a68690ec2c5c9b0b\" ) ? ; let network = \"testnet\" ; let scheduler = gl_client :: scheduler :: Scheduler ( node_id , network ) ? ; let node : gl_client :: node :: ClnClient = scheduler . schedule () ? ; from glclient import TlsConfig , Scheduler , cert , key = b '...' , b '...' node_id = bytes . fromhex ( \"02058e8b6c2ad363ec59aa136429256d745164c2bdc87f98f0a68690ec2c5c9b0b\" ) network = \"testnet\" tls = TlsConfig . with_identity ( cert , key ) scheduler = Scheduler ( node_id , network , tls ) node = scheduler . node () Once we have an instance of the Node we can start interacting with it via the GRPC interface: Rust Python use gl_client :: pb :: cln ; let info = node . get_info ( cln :: GetinfoRequest :: default ()). await ? ; let peers = node . list_peers ( gl_client :: pb :: cln :: ListpeersRequest :: default ()). await ? ; info = node . get_into () peers = node . list_peers () The above snippet will read the metadata and list the peers from the node. Both of these are read-only operations, that do not require a signer to sign off. What happens if we issue a command that requires a signer to sign off? Let's try to connect to create an invoice. Invoices are signed using the node key, and the signer is the only component with access to your key. Rust Python node . invoice ( cln :: InvoiceRequest { label : \"label\" . to_string (), description : \"description\" . to_string (), .. Default :: default (), }). await ? ; from glclient import clnpb node . invoice ( amount_msat = clnpb . AmountOrAny ( any = True ), label = \"label\" , description = \"description\" , ) You'll notice that these calls hang indefinitely. This is because the signer is not running and not attached to the node, and without its signature we can't create the invoice. This isn't just the case for the invoice call either, all calls that somehow use the Node ID, or move funds, will require the signer's sign-off. You can think of a node without a signer being connected as a read-only node, and as soon as you attach the signer, the node becomes fully functional. So how do we attach the signer? Simple: load the secret from where you stored it in the last chapter, instantiate the signer with it and then start it. Rust Python let seed = .. . // Load from wherever you stored it let ( cert , key ) = .. . // Load the cert and key you got from the `register` call // The signer task will run until we send a shutdown signal on this channel let ( tx , mut rx ) = tokio :: mpsc :: channel ( 1 ); let tls = TlsConfig (). identity ( cert , key ); signer = Signer ( seed , Network :: Bitcoin , tls ); signer . run_forever ( rx ). await ? ; Notice that signer.run_forever() returns a Future which you can spawn a new task with. That is also the reason why a separate shutdown signal is provided. seed = ... # Load from wherever you stored it cert , key = ... // Load the cert and key you got from the ` register ` call tls = TlsConfig () . with_identity ( res . device_cert , res . device_key ) signer = Signer :: new ( secret , Network :: Bitcoin , tls ) signer . run_in_thread () If you kept the stuck commands above running, you should notice that they now return a result. As mentioned before many RPC calls will need the signer to be attached to the node, so it's best to just start it early, and keep it running in the background whenever possible. The signer will not schedule the node by itself, instead waiting on the scheduler, so it doesn't consume much resources, but still be available when it is needed.","title":"Starting a node"},{"location":"reference/","text":"Overview","title":"Overview"},{"location":"reference/#overview","text":"","title":"Overview"},{"location":"reference/changelog/","text":"2023 May","title":"2023"},{"location":"reference/changelog/#2023","text":"","title":"2023"},{"location":"reference/changelog/#may","text":"","title":"May"},{"location":"reference/lsp/","text":"Lightning Service Provider integration Greenlight includes a client for the JIT LSP protocol. In particular the gl-plugin implements support for LSP JIT fees. These are fees leveraged as payment for a channel being opened, by forwarding a reduced amount, i.e., holding back a non-LN native fee. This is implemented on the LSP by intercepting and modifying the payload for the HTLC being forwarded, but this also causes the onion payload to mismatch the HTLC parameters (forwarded amount and total amount the sender intended to send). This would normally cause the recipient to fail the payment, however they are aware of this fee being leveraged, so we need to get the onion payload to match the HTLC and the corresponding invoice. The opt-in currently is determined by a matching invoice being present (payment_hash) and the sender values in the onion not matching the invoice. Hence the LSP needs to store a reduced invoice a the recipient node, but give out the original invoice to the prospective sender. Overview Todo Caveats The LSP has to have the ability to know the amount that the destination expects in order to satisfy that expectation. In order to do so we need to have a couple of values match up: Each HTLC value has to match or exceed the amt_to_forward field in the destination's onion payload. The total_msat value at the end of the payment_secret onion payload field has to match the sum of amt_to_forward values communicated in the HTLCs (on overpayment we reject, since the sender should have matched the expected amount exactly). The first is simple to patch in gl-plugin since we just need to inspect the two values and adjust the one in the onion payload to match. The latter is not, but we can glimpse the expected value by looking up the invoice that is being paid. That does not work in the following two cases: Spontaneous payments: don't have a matching invoice, and we can't decrypt the payload on the LSP to learn the amount the sender intended to deliver. Since the LSP uses routehints in invoices the sender would likely just not find a path. Amount-less invoices: same as above.","title":"LSP Integration"},{"location":"reference/lsp/#lightning-service-provider-integration","text":"Greenlight includes a client for the JIT LSP protocol. In particular the gl-plugin implements support for LSP JIT fees. These are fees leveraged as payment for a channel being opened, by forwarding a reduced amount, i.e., holding back a non-LN native fee. This is implemented on the LSP by intercepting and modifying the payload for the HTLC being forwarded, but this also causes the onion payload to mismatch the HTLC parameters (forwarded amount and total amount the sender intended to send). This would normally cause the recipient to fail the payment, however they are aware of this fee being leveraged, so we need to get the onion payload to match the HTLC and the corresponding invoice. The opt-in currently is determined by a matching invoice being present (payment_hash) and the sender values in the onion not matching the invoice. Hence the LSP needs to store a reduced invoice a the recipient node, but give out the original invoice to the prospective sender.","title":"Lightning Service Provider integration"},{"location":"reference/lsp/#overview","text":"Todo","title":"Overview"},{"location":"reference/lsp/#caveats","text":"The LSP has to have the ability to know the amount that the destination expects in order to satisfy that expectation. In order to do so we need to have a couple of values match up: Each HTLC value has to match or exceed the amt_to_forward field in the destination's onion payload. The total_msat value at the end of the payment_secret onion payload field has to match the sum of amt_to_forward values communicated in the HTLCs (on overpayment we reject, since the sender should have matched the expected amount exactly). The first is simple to patch in gl-plugin since we just need to inspect the two values and adjust the one in the onion payload to match. The latter is not, but we can glimpse the expected value by looking up the invoice that is being paid. That does not work in the following two cases: Spontaneous payments: don't have a matching invoice, and we can't decrypt the payload on the LSP to learn the amount the sender intended to deliver. Since the LSP uses routehints in invoices the sender would likely just not find a path. Amount-less invoices: same as above.","title":"Caveats"},{"location":"reference/partner-certs/","text":"Partner Certificates Greenlight includes an invite system that is used to control the influx of new users, and avoid overloading the system without sufficient time to scale up the infrastructure. In order to register a new node the client has to present an invite code when calling Scheduler.register() . The invite code can only be used once, and the plan is to regularly top up invites existing users can hand out. In order not to curtail your product's growth through these invitations, Greenlight also includes a bypass system through the partner certificates . These are custom certificates that partners can bundle with their application, and that allow registering new nodes without having to rely on invites. Impact of registration method on the node Independently of whether you registered your node with a partner certificate or an invite code, you'll get the same service. Nodes can be used from a multitude of different applications, independently of these applications using one mechanism or the other. Nodes that predate the introduction of the invite system. Who gets a Partner Certificate? As a rule of thumb, once your application has over 1'000 users, each with their own node, you'll want to ask us for a partner certificate. Upon contacting us we will verify that your application is suitable for a partnership with us, and provide you with the certificate to use. Suitability is dependent both on the application, as well as the overall status of Greenlight. Since this is a surge-protection mechanism for Greenlight, we cannot accept all applications, but we do our best. Once you have the certificate you can follow the instructions below. Using a Partner Certificate The partner certificate is a custom version of the /users/nobody certificate that is used to bootstrap the trust chain for clients that have not yet received their own private key and certificate specific to their node. As such the private key and certificate are compiled into gl-client at build time. The certificate is ditributed as two x509 PEM files bundled into an encrypted zip file: partner-{NAME}.pem : this is the certificate that the client will present when connecting to the Scheduler in order to either register() or recover() . partner-{NAME}-key.pem : this is the private key matching the above certificate and is used to encrypt the transport and authenticate as a partner to the Scheduler, allowing to bypass the invite code requirement. Alongside the encrypted zip file, you will also receive the password to decrypt the zip file. Ideally the two files are then stored, securely, alongside the code, in encrypted form, or instrument your CI system to have access to them when building. Treat the private key with the same care you'd use for an API key, as they fulfill identical roles in this scenario. In order to tell the build to use the partner certificate you'll have to set two environment variables in the build environment. Please consult your build system and/or shell environment about how to ensure teh build sees the variables. GL_CUSTOM_NOBODY_KEY should have the absolute path to partner-{NAME}-key.pem GL_CUSTOM_NOBODY_CERT should have the absolute path to partner-{NAME}.pem If either of these is not set you'll get a warning that this is an invite-based client: warning: Using default NOBODY cert. warning: Set \"GL_CUSTOM_NOBODY_KEY\" and \"GL_CUSTOM_NOBODY_CERT\" to use a custom cert. In case you When (not) to use the partner certificate In order to retain the protection aspect of the partner certificates please only use them in your own applications, and don't share them with others, directly or indirectly. In particular this means that you should not include them if you are building a library that others will use.","title":"Partner Certificates"},{"location":"reference/partner-certs/#partner-certificates","text":"Greenlight includes an invite system that is used to control the influx of new users, and avoid overloading the system without sufficient time to scale up the infrastructure. In order to register a new node the client has to present an invite code when calling Scheduler.register() . The invite code can only be used once, and the plan is to regularly top up invites existing users can hand out. In order not to curtail your product's growth through these invitations, Greenlight also includes a bypass system through the partner certificates . These are custom certificates that partners can bundle with their application, and that allow registering new nodes without having to rely on invites. Impact of registration method on the node Independently of whether you registered your node with a partner certificate or an invite code, you'll get the same service. Nodes can be used from a multitude of different applications, independently of these applications using one mechanism or the other. Nodes that predate the introduction of the invite system.","title":"Partner Certificates"},{"location":"reference/partner-certs/#who-gets-a-partner-certificate","text":"As a rule of thumb, once your application has over 1'000 users, each with their own node, you'll want to ask us for a partner certificate. Upon contacting us we will verify that your application is suitable for a partnership with us, and provide you with the certificate to use. Suitability is dependent both on the application, as well as the overall status of Greenlight. Since this is a surge-protection mechanism for Greenlight, we cannot accept all applications, but we do our best. Once you have the certificate you can follow the instructions below.","title":"Who gets a Partner Certificate?"},{"location":"reference/partner-certs/#using-a-partner-certificate","text":"The partner certificate is a custom version of the /users/nobody certificate that is used to bootstrap the trust chain for clients that have not yet received their own private key and certificate specific to their node. As such the private key and certificate are compiled into gl-client at build time. The certificate is ditributed as two x509 PEM files bundled into an encrypted zip file: partner-{NAME}.pem : this is the certificate that the client will present when connecting to the Scheduler in order to either register() or recover() . partner-{NAME}-key.pem : this is the private key matching the above certificate and is used to encrypt the transport and authenticate as a partner to the Scheduler, allowing to bypass the invite code requirement. Alongside the encrypted zip file, you will also receive the password to decrypt the zip file. Ideally the two files are then stored, securely, alongside the code, in encrypted form, or instrument your CI system to have access to them when building. Treat the private key with the same care you'd use for an API key, as they fulfill identical roles in this scenario. In order to tell the build to use the partner certificate you'll have to set two environment variables in the build environment. Please consult your build system and/or shell environment about how to ensure teh build sees the variables. GL_CUSTOM_NOBODY_KEY should have the absolute path to partner-{NAME}-key.pem GL_CUSTOM_NOBODY_CERT should have the absolute path to partner-{NAME}.pem If either of these is not set you'll get a warning that this is an invite-based client: warning: Using default NOBODY cert. warning: Set \"GL_CUSTOM_NOBODY_KEY\" and \"GL_CUSTOM_NOBODY_CERT\" to use a custom cert. In case you","title":"Using a Partner Certificate"},{"location":"reference/partner-certs/#when-not-to-use-the-partner-certificate","text":"In order to retain the protection aspect of the partner certificates please only use them in your own applications, and don't share them with others, directly or indirectly. In particular this means that you should not include them if you are building a library that others will use.","title":"When (not) to use the partner certificate"},{"location":"reference/security/","text":"Security Each component in the Greenlight system is uniquely identified by an mTLS keypair, also called an identity . These are either used directly to setup mutually authenticated TLS connections, or to sign payloads in cases direct connections are not desirable or possible. In addition the signer also has access to the Bitcoin keypair that backs the Node ID, as well as the on-chain wallet. We will refer to this keypair as the signer-identity , whereas mTLS keypairs are just called the client-identities . In the following scenarios we will consider an attacker that has access to the node infrastructure, but not the client or the signer. This can either be an external attacker or a rogue Greenlight operator. Our goal is to prevent any access to funds from such an attacker, whether internal or external, by checking authorization on both the node as well as the signer level. Client \u21c4 Node authentication This is a direct connection from a client to the node, or the signer to the node. The mTLS certificate hierarchy is under the control of the Greenlight Team. Each user gets their own CA, and nodes are configured to only accept client connections from certificates matching that CA. This guarantees that users can only contact their own node, while all other nodes would cause a mismatch and disconnect the client. Example Access to the node is currently all-or-nothing. The planned introduction of Rune-based access control will enable users to limit the operations a given client can execute. This is dependent on the pairing process as any client that has access to the signer could just escalate its privileges via the recovery. The private key for the client-identity is generated on the client itself, and never leaves the client, sending only a certificate signature request (CSR) to the scheduler which creates and signs the certificate. This puts the client in the correct subtree of the CA hierarchy, enabling it to contact the node. Impersonation by a potential attacker is prevented by keeping the private key for the client-identity on the client, and not share it with the server. Notice however that the Greenlight team, being in control of the CA hierarchy, could create a bogus client certificate and use that to issue commands to the node. More on this in the next section. Client \u21c4 Signer authentication The signer cannot rely on the mTLS CA structure, since that is under control of the Greenlight team. As such it uses an attestation scheme in which the signer-identity attests to itself (and other signers) that a given client is permitted to perform some operations. Upon registering a new client, the signer will provide an attestation signature. This attestation signature is what tells other signers that commands originating from that client are to be accepted. This is important because it allows multiple signers to recognize which clients are authorized, even if the attesting signer is not the signer that is currently verifying the authorization. Before signing a request the signer independently verifies that: The operations that it is asked to sign off on match pending RPC commands, and are safe to perform. The pending RPC commands are all signed by a valid client-identity The client-identities all have a matching attestation signature from the signer None of the pending RPC commands is a replay of a previously completed RPC command. An attacker that has gotten access to the node infrastructure may inject RPC commands directly into the node, side-stepping any authorization check on the node. For this reason the signer performs the same checks both on the node as well as the signer, the former preventing read-access that doesn't involve the signer, while the latter ensures funds are not moved without a client authorizing it. The client-identity pubkey, its signature of the command payload, and the signer-attestation are all passed to the node via grpc headers. The node extracts them, alongside the call itself, and adds it to a request context which will itself be attached to requests that are sent to the signer, so it can verify the validity and authenticity of the operations. An attacker that gains access to the node is unable to provide either these signatures and will therefore fail to convince the signer of its injected commands.","title":"Security"},{"location":"reference/security/#security","text":"Each component in the Greenlight system is uniquely identified by an mTLS keypair, also called an identity . These are either used directly to setup mutually authenticated TLS connections, or to sign payloads in cases direct connections are not desirable or possible. In addition the signer also has access to the Bitcoin keypair that backs the Node ID, as well as the on-chain wallet. We will refer to this keypair as the signer-identity , whereas mTLS keypairs are just called the client-identities . In the following scenarios we will consider an attacker that has access to the node infrastructure, but not the client or the signer. This can either be an external attacker or a rogue Greenlight operator. Our goal is to prevent any access to funds from such an attacker, whether internal or external, by checking authorization on both the node as well as the signer level.","title":"Security"},{"location":"reference/security/#client-node-authentication","text":"This is a direct connection from a client to the node, or the signer to the node. The mTLS certificate hierarchy is under the control of the Greenlight Team. Each user gets their own CA, and nodes are configured to only accept client connections from certificates matching that CA. This guarantees that users can only contact their own node, while all other nodes would cause a mismatch and disconnect the client. Example Access to the node is currently all-or-nothing. The planned introduction of Rune-based access control will enable users to limit the operations a given client can execute. This is dependent on the pairing process as any client that has access to the signer could just escalate its privileges via the recovery. The private key for the client-identity is generated on the client itself, and never leaves the client, sending only a certificate signature request (CSR) to the scheduler which creates and signs the certificate. This puts the client in the correct subtree of the CA hierarchy, enabling it to contact the node. Impersonation by a potential attacker is prevented by keeping the private key for the client-identity on the client, and not share it with the server. Notice however that the Greenlight team, being in control of the CA hierarchy, could create a bogus client certificate and use that to issue commands to the node. More on this in the next section.","title":"Client &rlarr; Node authentication"},{"location":"reference/security/#client-signer-authentication","text":"The signer cannot rely on the mTLS CA structure, since that is under control of the Greenlight team. As such it uses an attestation scheme in which the signer-identity attests to itself (and other signers) that a given client is permitted to perform some operations. Upon registering a new client, the signer will provide an attestation signature. This attestation signature is what tells other signers that commands originating from that client are to be accepted. This is important because it allows multiple signers to recognize which clients are authorized, even if the attesting signer is not the signer that is currently verifying the authorization. Before signing a request the signer independently verifies that: The operations that it is asked to sign off on match pending RPC commands, and are safe to perform. The pending RPC commands are all signed by a valid client-identity The client-identities all have a matching attestation signature from the signer None of the pending RPC commands is a replay of a previously completed RPC command. An attacker that has gotten access to the node infrastructure may inject RPC commands directly into the node, side-stepping any authorization check on the node. For this reason the signer performs the same checks both on the node as well as the signer, the former preventing read-access that doesn't involve the signer, while the latter ensures funds are not moved without a client authorizing it. The client-identity pubkey, its signature of the command payload, and the signer-attestation are all passed to the node via grpc headers. The node extracts them, alongside the call itself, and adds it to a request context which will itself be attached to requests that are sent to the signer, so it can verify the validity and authenticity of the operations. An attacker that gains access to the node is unable to provide either these signatures and will therefore fail to convince the signer of its injected commands.","title":"Client &rlarr; Signer authentication"},{"location":"tutorials/","text":"Tutorials Tutorials are self-contained walkthroughs, starting with a goal, and showing how to achieve this goal via step-by-step instructions. The aim of the tutorials in this section is to focus one aspect of Greenlight, from start to finish, and serve as an example of your own application. Note The information in these pages is primarily aimed at developers wanting to build on, or extend Greenlight, though end-users may find some of it interesting as well.","title":"Overview"},{"location":"tutorials/#tutorials","text":"Tutorials are self-contained walkthroughs, starting with a goal, and showing how to achieve this goal via step-by-step instructions. The aim of the tutorials in this section is to focus one aspect of Greenlight, from start to finish, and serve as an example of your own application. Note The information in these pages is primarily aimed at developers wanting to build on, or extend Greenlight, though end-users may find some of it interesting as well.","title":"Tutorials"},{"location":"tutorials/self-hosting/","text":"Self-Hosting Your Node It is our goal to help you get started with the Lightning Network. However once you learned all that is required to run your own node, it is time to level up and approach the next challenge: managing your own node. For this purpose Greenlight allows users to export their nodes and restore it on their own infrastructure. This process can take a couple of minutes to complete, but it is faster and cheaper than migrating by shutting down the old node on GL, transferring all your funds, and having to bootstrap a new node from scratch. Under the hood we will: Mark the node as exported, so it doesn't start on GL going forward. Initiate a DB backup of the node's wallet database. Encrypt the DB backup so that it can be decrypted only by users that have access to the seed phrase. Store the encrypted backup on a file store, from where it can be downloaded, decrypted and restored locally by the user. Notice that step 1, marking the node as not-schedulable, is important for the security of the funds: if the node on GL were allowed to make progress it'd invalidate the backup, which could lead to channel closures. Setup Your Infrastructure Before initiating the export, we will first need to prepare the new home where the node will be running going forward. The node running on Greenlight is a slightly modified Core Lightning node, with some of custom subdaemons and plugins. You can chose to replicate the same node by also running the custom subdaemons and plugins, or you can run a vanilla Core Lightning node. Custom components include the following: signerproxy subdaemon: exposes the signer interface to the signer, enabling the remote signer setup. gl-plugin plugin: exposes the plugin interface, including mTLS verification, and acts as a tailable signer request stream. If you'd like to continue using the remote signer you should use both gl-plugin and signerproxy in your own setup as well. If not you will have to create the hsm_secret file in the CLN config directory so that the stock CLN signer can find it and use it to run a local-only signer. If you have any clients configured you'll likely want to run the gl-plugin rather than the stock grpc-plugin , as the former implements a superset of the latter. In addition clients will still be able to reach the node through the node URL through the GL reverse proxy. In the next sections we provide step-by-step instructions to restore the node's database, and then setup the different variants. You will have to decide which one best suites your needs :wink: Restoring the database Todo Describe how to decrypt the backup and restore it into a new postgres database Remote Signer Setup Todo Describe how to adjust the lightningd service to use the signerproxy instead of the built-in hsmd subdaemon. Local Signer Setups Todo Should just work out of the box. Minimal Setup The minimal setup to host an exported node consists of a PostgreSQL database, and a CLN installation matching or newer than the version that was running on Greenlight. Assuming the database is running locally with the credentials pguser and pgpass you' you'll have to run CLN node like this: lightningd --wallet = postgres://pguser:pgpass@localhost:5432/dbname Initiate Export The export can be triggered via the Scheduler.export_node method on the Scheduler's grpc interface . This method call may take a couple of minutes depending on the age of the node, the number of channels opened and closed, as well as the number of payments sent and received. Upon successful export the call will return a URL from a file host where the encrypted backup can be downloaded. Warning Once the state of the node has been switched you will no longer be able to schedule the node on Greenlight's infrastructure. We do not allow re-activating the node because of the risk of a replica running somewhere else, which could cause loss of funds! Do not worry if the connection is lost during this process, as the method is idempotent and will complete in the background if the connection is lost. Calling the method multiple times will results in the same encrypted backup URL","title":"Self-Hosting"},{"location":"tutorials/self-hosting/#self-hosting-your-node","text":"It is our goal to help you get started with the Lightning Network. However once you learned all that is required to run your own node, it is time to level up and approach the next challenge: managing your own node. For this purpose Greenlight allows users to export their nodes and restore it on their own infrastructure. This process can take a couple of minutes to complete, but it is faster and cheaper than migrating by shutting down the old node on GL, transferring all your funds, and having to bootstrap a new node from scratch. Under the hood we will: Mark the node as exported, so it doesn't start on GL going forward. Initiate a DB backup of the node's wallet database. Encrypt the DB backup so that it can be decrypted only by users that have access to the seed phrase. Store the encrypted backup on a file store, from where it can be downloaded, decrypted and restored locally by the user. Notice that step 1, marking the node as not-schedulable, is important for the security of the funds: if the node on GL were allowed to make progress it'd invalidate the backup, which could lead to channel closures.","title":"Self-Hosting Your Node"},{"location":"tutorials/self-hosting/#setup-your-infrastructure","text":"Before initiating the export, we will first need to prepare the new home where the node will be running going forward. The node running on Greenlight is a slightly modified Core Lightning node, with some of custom subdaemons and plugins. You can chose to replicate the same node by also running the custom subdaemons and plugins, or you can run a vanilla Core Lightning node. Custom components include the following: signerproxy subdaemon: exposes the signer interface to the signer, enabling the remote signer setup. gl-plugin plugin: exposes the plugin interface, including mTLS verification, and acts as a tailable signer request stream. If you'd like to continue using the remote signer you should use both gl-plugin and signerproxy in your own setup as well. If not you will have to create the hsm_secret file in the CLN config directory so that the stock CLN signer can find it and use it to run a local-only signer. If you have any clients configured you'll likely want to run the gl-plugin rather than the stock grpc-plugin , as the former implements a superset of the latter. In addition clients will still be able to reach the node through the node URL through the GL reverse proxy. In the next sections we provide step-by-step instructions to restore the node's database, and then setup the different variants. You will have to decide which one best suites your needs :wink:","title":"Setup Your Infrastructure"},{"location":"tutorials/self-hosting/#restoring-the-database","text":"Todo Describe how to decrypt the backup and restore it into a new postgres database","title":"Restoring the database"},{"location":"tutorials/self-hosting/#remote-signer-setup","text":"Todo Describe how to adjust the lightningd service to use the signerproxy instead of the built-in hsmd subdaemon.","title":"Remote Signer Setup"},{"location":"tutorials/self-hosting/#local-signer-setups","text":"Todo Should just work out of the box.","title":"Local Signer Setups"},{"location":"tutorials/self-hosting/#minimal-setup","text":"The minimal setup to host an exported node consists of a PostgreSQL database, and a CLN installation matching or newer than the version that was running on Greenlight. Assuming the database is running locally with the credentials pguser and pgpass you' you'll have to run CLN node like this: lightningd --wallet = postgres://pguser:pgpass@localhost:5432/dbname","title":"Minimal Setup"},{"location":"tutorials/self-hosting/#initiate-export","text":"The export can be triggered via the Scheduler.export_node method on the Scheduler's grpc interface . This method call may take a couple of minutes depending on the age of the node, the number of channels opened and closed, as well as the number of payments sent and received. Upon successful export the call will return a URL from a file host where the encrypted backup can be downloaded. Warning Once the state of the node has been switched you will no longer be able to schedule the node on Greenlight's infrastructure. We do not allow re-activating the node because of the risk of a replica running somewhere else, which could cause loss of funds! Do not worry if the connection is lost during this process, as the method is idempotent and will complete in the background if the connection is lost. Calling the method multiple times will results in the same encrypted backup URL","title":"Initiate Export"},{"location":"tutorials/testing/","text":"Testing with gl-testing The repository includes a testing framework that can be used to test your application locally. This framework is built on top of pyln-testing (also used to develop Core Lightning itself), allowing developers to describe arbitrarily complex network setups, alongside a mock Greenlight service, and test their functionality in a repeatable and reproducible way. If you are already familiar with pytest you should feel right at home, if not, don't worry, we will walk through an example together here. Why test and what to test? You have just written the next viral app on top of Greenlight, how do you ensure it a) works now, and b) keeps on working going forward? You could manually test your application after each change against the Greenlight servers, or you might even automate some of these, but they still run against the production environment. On one hand this is likely not as fast as you're used to for local tests, and it actually allocates resources on the service that someone will have to pay for. Clearly it'd be better if we had a mock implementation of Greenluight that you can run locally, and that can be torn down in order to free the resources. This is pretty much what gl-testing is for. It consists of a mock python implementation of the Scheduler , on top of all the existing utilities from pyln-testing , and is bundled in a simple to use docker image. The docker image comes with all the dependencies installed, such as multiple CLN versions to test against, and is pre-configured to create an environment that is as close to the production setup as possible. Differences between gl-testing and the production environment We keep track of when behavior of gl-testing diverges substantially from the production behavior in the gl-testing readme Testing in the docker images is optional for Linux hosts, but strongly suggested due to the rather large number of dependencies. For Windows and MacOS we only support testing in the docker image, since the pre-compiled CLN versions are compiled for Linux on x86_64 only for now. Self-testing You will probably have expected this, but we also use gl-testing to test the bindings themselves. If you are working on a pull request for gl-client or one of the other components it is strongly suggested that you test your changes prior to submitting. To do so you can use a couple of Makefile targets: # Create the `gltesting` docker image make docker-image # Open an interactive shell in an instance of `gltesting` make docker-shell Then from inside the docker-shell you can invoke some more commands: # Build the `gl-client-py` bindings and install them in our python environment make build-self # Run the tests in `libs/gl-testing/tests` against the framework make check-self Warning Depending on you system setup you may need to prefix these commands with sudo or become root first. This is because docker , which is used by those Makefile targets requires the user to either be in the docker group, or be root . Writing your own test Tests in gl-testing work best if you have a programmatic way of driving your client. This could either be your own testing framework, e.g., having a method to trigger a button press in your UI, or by exposing your own API. In this example we will walk through a simple test that Sets up a small test network Starts a Greenlight node Opens a channel from the network to the greenlight node Creates an invoice on the greenlight node Pays the invoice from a node in the network Here's the example code in its entirety and we will walk through the individual parts afterwards: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def test_node_network ( node_factory , clients , bitcoind ): \"\"\"Setup a small network and check that we can send/receive payments. \"\"\" l1 , l2 = node_factory . line_graph ( 2 ) c = clients . new () c . register ( configure = True ) gl1 = c . node () # Handshake needs signer for ECDH of Noise_XK exchange s = c . signer () . run_in_thread () gl1 . connect_peer ( l2 . info [ 'id' ], f '127.0.0.1: { l2 . daemon . port } ' ) # Now open a channel from l2 -> gl1 l2 . fundwallet ( sats = 2 * 10 ** 6 ) l2 . rpc . fundchannel ( c . node_id . hex (), 'all' ) bitcoind . generate_block ( 1 , wait_for_mempool = 1 ) # Now wait for the channel to confirm wait_for ( lambda : gl1 . list_peers () . peers [ 0 ] . channels [ 0 ] . state == 'CHANNELD_NORMAL' ) inv = gl1 . create_invoice ( 'test' , nodepb . Amount ( millisatoshi = 10000 ) ) . bolt11 l1 . rpc . pay ( inv ) Line 1: we define a new test: 1 def test_node_network ( node_factory , clients , bitcoind ): Any function that starts with test_ will be picked up by the test runner and executed. The arguments are fixtures that can be users. More on fixtures further down. Next we create a small network of two nodes, l1 and l2 , connected by a channel using the node_factory fixture. This fixture is used to start and control non-greenlight nodes. 5 l1 , l2 = node_factory . line_graph ( 2 ) Using the clients fixture we create a new client, along with its own directory, signer secret, and certificates. We then use Client.register() to register the node with Greenlight and then schedule it right away with the call to Client.node() 7 8 9 c = clients . new () c . register ( configure = True ) gl1 = c . node () Lines 14-21: We start the signer (required to complete the handshake and any other signer request we will encounter), connect outwards from the GL node to l2 representing the rest of the network, and then using l2 to fund a channel over the connection we just opened. 11 12 13 14 15 16 17 18 19 # Handshake needs signer for ECDH of Noise_XK exchange s = c . signer () . run_in_thread () gl1 . connect_peer ( l2 . info [ 'id' ], f '127.0.0.1: { l2 . daemon . port } ' ) # Now open a channel from l2 -> gl1 l2 . fundwallet ( sats = 2 * 10 ** 6 ) l2 . rpc . fundchannel ( c . node_id . hex (), 'all' ) bitcoind . generate_block ( 1 , wait_for_mempool = 1 ) Notice that the last step waits for the funding transaction and confirms it. We then need to wait for the two nodes to notice that the funding transaction was confirmed before continuing: 20 21 # Now wait for the channel to confirm wait_for ( lambda : gl1 . list_peers () . peers [ 0 ] . channels [ 0 ] . state == 'CHANNELD_NORMAL' ) This will poll gl1 for its channel states and return as soon as the state indicates that the channel is confirmed and fully functional. Finally we can create an invoice from the Greenlight node gl1 and pay it using l1 , which should result in a path l1 -> l2 -> gl1 being used, and the invoice will be paid after this: 23 24 25 26 27 28 inv = gl1 . create_invoice ( 'test' , nodepb . Amount ( millisatoshi = 10000 ) ) . bolt11 l1 . rpc . pay ( inv ) To run this test you can use the above Makefile target: $ sudo make docker-image docker-shell # (1)! $ make build-self check-self # (2)! Enter the docker shell, building the image if it was not generated before. In the docker shell, build gl-client and bindings, then execute the self-tests If you can drive your application from a python script through a UI testing framework or an API, you can just use the above test as a template and start writing your own tests. If you don't have a way to run your application through a scripted test you may want to start a network, and test against that manually. For an example of this see the next section. Manually testing against a mock Network Every once in a while you'll want to either step through an existing test, or have a small test that just sets up a network topology, and then drops you in a shell that you can use to interact with the network. In both cases breakpoint() is your friend The following test will set up the small network above, and then drop you in a REPL that you can use to inspect the setup, and to drive changes such as paying an invoice or funding a channel: from gltesting.fixtures import * import os def test_my_network ( clients , node_factory , scheduler , directory , bitcoind ): \"\"\"Start a small line_graph network to play with. \"\"\" l1 , l2 , l3 = node_factory . line_graph ( 3 ) # (1)! # Assuming we want interact with l3 we'll want to print # its contact details: print ( f \"scheduler: https://localhost: { scheduler . grpc_port } \" ) # (3)! print ( f \"l3 details: { l3 . info [ 'id' ] } @ 127.0.0.1: { l3 . daemon . port } \" ) print ( f \"CA Cert: { directory } /certs/ca.pem\" ) # (4) print ( f \"Nobody Cert: { directory } /certs/users/nobody.pem\" ) print ( f \"Nobody Key: { directory } /certs/users/nobody-key.pem\" ) breakpoint () # (2)! At this point we have a network with 3 nodes in a line. Opens a REPL that accepts Python code. Tells us which port the mock scheduler is listening on Prints the location of the keypairs and certificates to use when talking to the mock scheduler To start this test just run: $ pytest testy.py -s ========== test session starts ========== platform linux -- Python 3 .8.10, pytest-7.2.1, pluggy-1.0.0 rootdir: /repo plugins: cov-3.0.0, xdist-2.5.0, forked-1.6.0, timeout-2.1.0 collected 1 item testy.py Running tests in /tmp/ltests-syfsnw83 [ ... many more lines about the setup of the network ... ] scheduler: https://localhost:43841 l3 details: 035d2b1192dfba134e10e540875d366ebc8bc353d5aa766b80c090b39c3a5d885d @ 127 .0.0.1:34547 CA Cert: /tmp/gltesting/tmpfo6c2ye2/certs/ca.pem Nobody Cert: /tmp/gltesting/tmpfo6c2ye2/certs/users/nobody.pem Nobody Key: /tmp/gltesting/tmpfo6c2ye2/certs/users/nobody-key.pem >>>>>>>>>> PDB set_trace >>>>>>>>>> --Return-- > /repo/testy.py ( 20 ) test_my_network () ->None -> breakpoint () ( pdb ) At this point you will have a REPL that you can use to drive changes, by writing python code, just like you'd do if you were writing the test in a file. In order to attach your client application to the mock scheduler we need to set a number of environment variables that gl-client will pick up and use: export GL_NOBODY_CERT = /tmp/gltesting/tmpb7711nx5/certs/users/nobody.crt export GL_NOBODY_KEY = /tmp/gltesting/tmpb7711nx5/certs/users/nobody-key.pem export GL_CA_CRT = /tmp/gltesting/tmpb7711nx5/certs/ca.pem export GL_SCHEDULER_GRPC_URI = http://127.0.0.1:43841 The first three lines tell the client library which identity to load itself, and how to verify the identity of the scheduler when connecting. These must match the lines printed above. The last line tells the client to connect to our mock scheduler instead of the production scheduler, the port must match the one printed above. Why is this random? We usually run tests in parallel, which requires that we isolate the tests from each other. If we did not randomize the ports and directories, we could end up with tests that interfere with each other, making debugging much harder, and resulting in flaky tests. All of this works thanks to us mounting the /tmp/gltesting directory from the host, allowing both docker container and host to exchange files, and docker-shell also reuses the host network, allowing clients running on the host to talk directly to the scheduler and nodes, without having to change the IP and setup port-forwarding. Once you are done testing, use continue or Ctrl-D in the REPL to trigger a shutdown. Fixtures TODO","title":"Testing your app"},{"location":"tutorials/testing/#testing-with-gl-testing","text":"The repository includes a testing framework that can be used to test your application locally. This framework is built on top of pyln-testing (also used to develop Core Lightning itself), allowing developers to describe arbitrarily complex network setups, alongside a mock Greenlight service, and test their functionality in a repeatable and reproducible way. If you are already familiar with pytest you should feel right at home, if not, don't worry, we will walk through an example together here.","title":"Testing with gl-testing"},{"location":"tutorials/testing/#why-test-and-what-to-test","text":"You have just written the next viral app on top of Greenlight, how do you ensure it a) works now, and b) keeps on working going forward? You could manually test your application after each change against the Greenlight servers, or you might even automate some of these, but they still run against the production environment. On one hand this is likely not as fast as you're used to for local tests, and it actually allocates resources on the service that someone will have to pay for. Clearly it'd be better if we had a mock implementation of Greenluight that you can run locally, and that can be torn down in order to free the resources. This is pretty much what gl-testing is for. It consists of a mock python implementation of the Scheduler , on top of all the existing utilities from pyln-testing , and is bundled in a simple to use docker image. The docker image comes with all the dependencies installed, such as multiple CLN versions to test against, and is pre-configured to create an environment that is as close to the production setup as possible. Differences between gl-testing and the production environment We keep track of when behavior of gl-testing diverges substantially from the production behavior in the gl-testing readme Testing in the docker images is optional for Linux hosts, but strongly suggested due to the rather large number of dependencies. For Windows and MacOS we only support testing in the docker image, since the pre-compiled CLN versions are compiled for Linux on x86_64 only for now.","title":"Why test and what to test?"},{"location":"tutorials/testing/#self-testing","text":"You will probably have expected this, but we also use gl-testing to test the bindings themselves. If you are working on a pull request for gl-client or one of the other components it is strongly suggested that you test your changes prior to submitting. To do so you can use a couple of Makefile targets: # Create the `gltesting` docker image make docker-image # Open an interactive shell in an instance of `gltesting` make docker-shell Then from inside the docker-shell you can invoke some more commands: # Build the `gl-client-py` bindings and install them in our python environment make build-self # Run the tests in `libs/gl-testing/tests` against the framework make check-self Warning Depending on you system setup you may need to prefix these commands with sudo or become root first. This is because docker , which is used by those Makefile targets requires the user to either be in the docker group, or be root .","title":"Self-testing"},{"location":"tutorials/testing/#writing-your-own-test","text":"Tests in gl-testing work best if you have a programmatic way of driving your client. This could either be your own testing framework, e.g., having a method to trigger a button press in your UI, or by exposing your own API. In this example we will walk through a simple test that Sets up a small test network Starts a Greenlight node Opens a channel from the network to the greenlight node Creates an invoice on the greenlight node Pays the invoice from a node in the network Here's the example code in its entirety and we will walk through the individual parts afterwards: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def test_node_network ( node_factory , clients , bitcoind ): \"\"\"Setup a small network and check that we can send/receive payments. \"\"\" l1 , l2 = node_factory . line_graph ( 2 ) c = clients . new () c . register ( configure = True ) gl1 = c . node () # Handshake needs signer for ECDH of Noise_XK exchange s = c . signer () . run_in_thread () gl1 . connect_peer ( l2 . info [ 'id' ], f '127.0.0.1: { l2 . daemon . port } ' ) # Now open a channel from l2 -> gl1 l2 . fundwallet ( sats = 2 * 10 ** 6 ) l2 . rpc . fundchannel ( c . node_id . hex (), 'all' ) bitcoind . generate_block ( 1 , wait_for_mempool = 1 ) # Now wait for the channel to confirm wait_for ( lambda : gl1 . list_peers () . peers [ 0 ] . channels [ 0 ] . state == 'CHANNELD_NORMAL' ) inv = gl1 . create_invoice ( 'test' , nodepb . Amount ( millisatoshi = 10000 ) ) . bolt11 l1 . rpc . pay ( inv ) Line 1: we define a new test: 1 def test_node_network ( node_factory , clients , bitcoind ): Any function that starts with test_ will be picked up by the test runner and executed. The arguments are fixtures that can be users. More on fixtures further down. Next we create a small network of two nodes, l1 and l2 , connected by a channel using the node_factory fixture. This fixture is used to start and control non-greenlight nodes. 5 l1 , l2 = node_factory . line_graph ( 2 ) Using the clients fixture we create a new client, along with its own directory, signer secret, and certificates. We then use Client.register() to register the node with Greenlight and then schedule it right away with the call to Client.node() 7 8 9 c = clients . new () c . register ( configure = True ) gl1 = c . node () Lines 14-21: We start the signer (required to complete the handshake and any other signer request we will encounter), connect outwards from the GL node to l2 representing the rest of the network, and then using l2 to fund a channel over the connection we just opened. 11 12 13 14 15 16 17 18 19 # Handshake needs signer for ECDH of Noise_XK exchange s = c . signer () . run_in_thread () gl1 . connect_peer ( l2 . info [ 'id' ], f '127.0.0.1: { l2 . daemon . port } ' ) # Now open a channel from l2 -> gl1 l2 . fundwallet ( sats = 2 * 10 ** 6 ) l2 . rpc . fundchannel ( c . node_id . hex (), 'all' ) bitcoind . generate_block ( 1 , wait_for_mempool = 1 ) Notice that the last step waits for the funding transaction and confirms it. We then need to wait for the two nodes to notice that the funding transaction was confirmed before continuing: 20 21 # Now wait for the channel to confirm wait_for ( lambda : gl1 . list_peers () . peers [ 0 ] . channels [ 0 ] . state == 'CHANNELD_NORMAL' ) This will poll gl1 for its channel states and return as soon as the state indicates that the channel is confirmed and fully functional. Finally we can create an invoice from the Greenlight node gl1 and pay it using l1 , which should result in a path l1 -> l2 -> gl1 being used, and the invoice will be paid after this: 23 24 25 26 27 28 inv = gl1 . create_invoice ( 'test' , nodepb . Amount ( millisatoshi = 10000 ) ) . bolt11 l1 . rpc . pay ( inv ) To run this test you can use the above Makefile target: $ sudo make docker-image docker-shell # (1)! $ make build-self check-self # (2)! Enter the docker shell, building the image if it was not generated before. In the docker shell, build gl-client and bindings, then execute the self-tests If you can drive your application from a python script through a UI testing framework or an API, you can just use the above test as a template and start writing your own tests. If you don't have a way to run your application through a scripted test you may want to start a network, and test against that manually. For an example of this see the next section.","title":"Writing your own test"},{"location":"tutorials/testing/#manually-testing-against-a-mock-network","text":"Every once in a while you'll want to either step through an existing test, or have a small test that just sets up a network topology, and then drops you in a shell that you can use to interact with the network. In both cases breakpoint() is your friend The following test will set up the small network above, and then drop you in a REPL that you can use to inspect the setup, and to drive changes such as paying an invoice or funding a channel: from gltesting.fixtures import * import os def test_my_network ( clients , node_factory , scheduler , directory , bitcoind ): \"\"\"Start a small line_graph network to play with. \"\"\" l1 , l2 , l3 = node_factory . line_graph ( 3 ) # (1)! # Assuming we want interact with l3 we'll want to print # its contact details: print ( f \"scheduler: https://localhost: { scheduler . grpc_port } \" ) # (3)! print ( f \"l3 details: { l3 . info [ 'id' ] } @ 127.0.0.1: { l3 . daemon . port } \" ) print ( f \"CA Cert: { directory } /certs/ca.pem\" ) # (4) print ( f \"Nobody Cert: { directory } /certs/users/nobody.pem\" ) print ( f \"Nobody Key: { directory } /certs/users/nobody-key.pem\" ) breakpoint () # (2)! At this point we have a network with 3 nodes in a line. Opens a REPL that accepts Python code. Tells us which port the mock scheduler is listening on Prints the location of the keypairs and certificates to use when talking to the mock scheduler To start this test just run: $ pytest testy.py -s ========== test session starts ========== platform linux -- Python 3 .8.10, pytest-7.2.1, pluggy-1.0.0 rootdir: /repo plugins: cov-3.0.0, xdist-2.5.0, forked-1.6.0, timeout-2.1.0 collected 1 item testy.py Running tests in /tmp/ltests-syfsnw83 [ ... many more lines about the setup of the network ... ] scheduler: https://localhost:43841 l3 details: 035d2b1192dfba134e10e540875d366ebc8bc353d5aa766b80c090b39c3a5d885d @ 127 .0.0.1:34547 CA Cert: /tmp/gltesting/tmpfo6c2ye2/certs/ca.pem Nobody Cert: /tmp/gltesting/tmpfo6c2ye2/certs/users/nobody.pem Nobody Key: /tmp/gltesting/tmpfo6c2ye2/certs/users/nobody-key.pem >>>>>>>>>> PDB set_trace >>>>>>>>>> --Return-- > /repo/testy.py ( 20 ) test_my_network () ->None -> breakpoint () ( pdb ) At this point you will have a REPL that you can use to drive changes, by writing python code, just like you'd do if you were writing the test in a file. In order to attach your client application to the mock scheduler we need to set a number of environment variables that gl-client will pick up and use: export GL_NOBODY_CERT = /tmp/gltesting/tmpb7711nx5/certs/users/nobody.crt export GL_NOBODY_KEY = /tmp/gltesting/tmpb7711nx5/certs/users/nobody-key.pem export GL_CA_CRT = /tmp/gltesting/tmpb7711nx5/certs/ca.pem export GL_SCHEDULER_GRPC_URI = http://127.0.0.1:43841 The first three lines tell the client library which identity to load itself, and how to verify the identity of the scheduler when connecting. These must match the lines printed above. The last line tells the client to connect to our mock scheduler instead of the production scheduler, the port must match the one printed above. Why is this random? We usually run tests in parallel, which requires that we isolate the tests from each other. If we did not randomize the ports and directories, we could end up with tests that interfere with each other, making debugging much harder, and resulting in flaky tests. All of this works thanks to us mounting the /tmp/gltesting directory from the host, allowing both docker container and host to exchange files, and docker-shell also reuses the host network, allowing clients running on the host to talk directly to the scheduler and nodes, without having to change the IP and setup port-forwarding. Once you are done testing, use continue or Ctrl-D in the REPL to trigger a shutdown.","title":"Manually testing against a mock Network"},{"location":"tutorials/testing/#fixtures","text":"TODO","title":"Fixtures"}]}